{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df = spark.sql('''select 'spark' as hello ''')\n",
    "df.show()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "appName = \"ReLU\"\n",
    "conf = SparkConf().setAppName(appName).setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([StructField(\"review_id\", StringType(), True), StructField(\"date\", TimestampType(), True), StructField(\"user_id\", StringType(), True), StructField(\"business_id\", StringType(), True), StructField(\"stars\", DoubleType(), True)])\n",
    "\n",
    "data_set = spark.read.json(\"./mini_review.json\", schema=schema, timestampFormat=\"yyyy-MM-dd HH:mm:ss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.createOrReplaceTempView(\"reviews\")\n",
    "last_data_set = spark.sql(\"SELECT R1.review_id, R1.date, R1.user_id, R1.business_id, R1.stars FROM reviews AS R1 JOIN (SELECT max(R2.date) AS last_date, R2.user_id, R2.business_id FROM reviews AS R2 GROUP BY R2.user_id, R2.business_id) table2 ON R1.date=table2.last_date AND R1.user_id=table2.user_id AND R1.business_id=table2.business_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99836"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.select(\"business_id\", \"user_id\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_users = np.concatenate(\n",
    "        last_data_set.select(\"user_id\").distinct().rdd.glom().map(\n",
    "          lambda x: np.array([elem[0] for elem in x]))\n",
    "        .collect())\n",
    "\n",
    "dictOfUsers = { unique_users[i] : i for i in range(0, len(unique_users) ) }\n",
    "\n",
    "unique_business = np.concatenate(\n",
    "        last_data_set.select(\"business_id\").distinct().rdd.glom().map(\n",
    "          lambda x: np.array([elem[0] for elem in x]))\n",
    "        .collect())\n",
    "\n",
    "dictOfBusiness = { unique_business[i] : i for i in range(0, len(unique_business) ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def translate(mapping):\n",
    "    def translate_(col):\n",
    "        return mapping.get(col)\n",
    "    return udf(translate_, StringType())\n",
    "\n",
    "last_data_set2 = last_data_set.withColumn('int_id_user', translate(dictOfUsers)('user_id'))\n",
    "last_data_set2 = last_data_set2.withColumn('int_id_business', translate(dictOfBusiness)('business_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= last_data_set2.select(\"int_id_user\", \"int_id_business\", \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[int_id_user: string, int_id_business: string, stars: double]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.persist(pyspark.StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[77] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = temp.rdd.map(lambda x: (int(x[0]), int(x[1]), x[2]))\n",
    "tuples.persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16081, 4680, 5.0),\n",
       " (46249, 12102, 5.0),\n",
       " (51911, 1921, 4.0),\n",
       " (67241, 11972, 3.0),\n",
       " (59105, 9979, 4.0),\n",
       " (79531, 3403, 4.0),\n",
       " (5157, 1407, 5.0),\n",
       " (44602, 712, 4.0),\n",
       " (54734, 4802, 3.0),\n",
       " (4761, 5002, 4.0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "smat=CoordinateMatrix(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79906 13951\n"
     ]
    }
   ],
   "source": [
    "m = smat.numRows()  # 3\n",
    "n = smat.numCols()  # 2\n",
    "print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Dream team project",
  "notebookId": 4050719103777520
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
